{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Project 1: Home Credit Default Risk (Binary Classification)\n",
                "\n",
                "## Objective\n",
                "Predict whether a client will default on a loan using the Home Credit Default Risk dataset.\n",
                "This notebook covers:\n",
                "1. Data Loading\n",
                "2. Exploratory Data Analysis (EDA)\n",
                "3. Preprocessing\n",
                "4. Building a Deep ANN (7-8 Layers)\n",
                "5. Hyperparameter Tuning\n",
                "6. Training and Evaluation\n",
                "7. Saving the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q keras-tuner"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import keras_tuner as kt\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "url = \"https://raw.githubusercontent.com/rmaso/home-credit-default-risk/master/application_train.csv\"\n",
                "df = pd.read_csv(url)\n",
                "print(f\"Dataset Shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check target distribution\n",
                "plt.figure(figsize=(6, 4))\n",
                "sns.countplot(x='TARGET', data=df)\n",
                "plt.title('Target Distribution (0: No Default, 1: Default)')\n",
                "plt.show()\n",
                "\n",
                "print(df['TARGET'].value_counts(normalize=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Missing Values\n",
                "missing_values = df.isnull().mean() * 100\n",
                "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
                "print(\"Top 10 features with missing values (%):\")\n",
                "print(missing_values.head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop columns with too many missing values (>50%)\n",
                "cols_to_drop = missing_values[missing_values > 50].index\n",
                "df = df.drop(columns=cols_to_drop)\n",
                "print(f\"Shape after dropping columns: {df.shape}\")\n",
                "\n",
                "# Drop ID column\n",
                "if 'SK_ID_CURR' in df.columns:\n",
                "    df = df.drop(columns=['SK_ID_CURR'])\n",
                "\n",
                "# Separate Target\n",
                "X = df.drop(columns=['TARGET'])\n",
                "y = df['TARGET']\n",
                "\n",
                "# Handle Categorical and Numerical Columns\n",
                "cat_cols = X.select_dtypes(include=['object']).columns\n",
                "num_cols = X.select_dtypes(include=['number']).columns\n",
                "\n",
                "# Impute Missing Values\n",
                "# Numerical: Median\n",
                "imputer_num = SimpleImputer(strategy='median')\n",
                "X[num_cols] = imputer_num.fit_transform(X[num_cols])\n",
                "\n",
                "# Categorical: Most Frequent\n",
                "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
                "X[cat_cols] = imputer_cat.fit_transform(X[cat_cols])\n",
                "\n",
                "# Encode Categorical Variables\n",
                "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
                "\n",
                "print(f\"Final Feature Shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split Data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "# Scale Data\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4 & 5. Build ANN & Hyperparameter Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_model(hp):\n",
                "    model = keras.Sequential()\n",
                "    model.add(layers.Input(shape=(X_train_scaled.shape[1],)))\n",
                "    \n",
                "    # Layer 0\n",
                "    model.add(layers.Dense(units=hp.Int('units_0', min_value=32, max_value=256, step=32), activation='relu'))\n",
                "    model.add(layers.Dropout(hp.Float('dropout_0', 0.0, 0.5, step=0.1)))\n",
                "    if hp.Boolean('batch_norm_0'):\n",
                "        model.add(layers.BatchNormalization())\n",
                "    \n",
                "    # Layer 1\n",
                "    model.add(layers.Dense(units=hp.Int('units_1', min_value=32, max_value=256, step=32), activation='relu'))\n",
                "    model.add(layers.Dropout(hp.Float('dropout_1', 0.0, 0.5, step=0.1)))\n",
                "    if hp.Boolean('batch_norm_1'):\n",
                "        model.add(layers.BatchNormalization())\n",
                "    \n",
                "    # Layer 2\n",
                "    model.add(layers.Dense(units=hp.Int('units_2', min_value=32, max_value=256, step=32), activation='relu'))\n",
                "    model.add(layers.Dropout(hp.Float('dropout_2', 0.0, 0.5, step=0.1)))\n",
                "    if hp.Boolean('batch_norm_2'):\n",
                "        model.add(layers.BatchNormalization())\n",
                "    \n",
                "    # Layer 3\n",
                "    model.add(layers.Dense(units=hp.Int('units_3', min_value=32, max_value=256, step=32), activation='relu'))\n",
                "    model.add(layers.Dropout(hp.Float('dropout_3', 0.0, 0.5, step=0.1)))\n",
                "    if hp.Boolean('batch_norm_3'):\n",
                "        model.add(layers.BatchNormalization())\n",
                "    \n",
                "    # Layer 4\n",
                "    model.add(layers.Dense(units=hp.Int('units_4', min_value=32, max_value=256, step=32), activation='relu'))\n",
                "    model.add(layers.Dropout(hp.Float('dropout_4', 0.0, 0.5, step=0.1)))\n",
                "    if hp.Boolean('batch_norm_4'):\n",
                "        model.add(layers.BatchNormalization())\n",
                "    \n",
                "    # Layer 5\n",
                "    model.add(layers.Dense(units=hp.Int('units_5', min_value=32, max_value=256, step=32), activation='relu'))\n",
                "    model.add(layers.Dropout(hp.Float('dropout_5', 0.0, 0.5, step=0.1)))\n",
                "    if hp.Boolean('batch_norm_5'):\n",
                "        model.add(layers.BatchNormalization())\n",
                "    \n",
                "    # Layer 6\n",
                "    model.add(layers.Dense(units=hp.Int('units_6', min_value=32, max_value=256, step=32), activation='relu'))\n",
                "    model.add(layers.Dropout(hp.Float('dropout_6', 0.0, 0.5, step=0.1)))\n",
                "    if hp.Boolean('batch_norm_6'):\n",
                "        model.add(layers.BatchNormalization())\n",
                "    \n",
                "    # Layer 7 (Conditional)\n",
                "    if hp.Int('num_layers', 7, 8) >= 8:\n",
                "        model.add(layers.Dense(units=hp.Int('units_7', min_value=32, max_value=256, step=32), activation='relu'))\n",
                "        model.add(layers.Dropout(hp.Float('dropout_7', 0.0, 0.5, step=0.1)))\n",
                "        if hp.Boolean('batch_norm_7'):\n",
                "            model.add(layers.BatchNormalization())\n",
                "            \n",
                "    # Output Layer (Binary Classification)\n",
                "    model.add(layers.Dense(1, activation='sigmoid'))\n",
                "    \n",
                "    # Compile\n",
                "    learning_rate = hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='log')\n",
                "    model.compile(\n",
                "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
                "        loss='binary_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tuner = kt.RandomSearch(\n",
                "    build_model,\n",
                "    objective='val_accuracy',\n",
                "    max_trials=5,\n",
                "    executions_per_trial=1,\n",
                "    directory='my_dir',\n",
                "    project_name='home_credit_tuning'\n",
                ")\n",
                "\n",
                "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
                "\n",
                "tuner.search(X_train_scaled, y_train, epochs=20, validation_split=0.2, callbacks=[stop_early])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get Best Hyperparameters\n",
                "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
                "\n",
                "print(f\"Best Number of Layers: {best_hps.get('num_layers')}\")\n",
                "print(f\"Best Learning Rate: {best_hps.get('lr')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = tuner.hypermodel.build(best_hps)\n",
                "\n",
                "history = model.fit(\n",
                "    X_train_scaled, \n",
                "    y_train, \n",
                "    epochs=100, \n",
                "    validation_split=0.2,\n",
                "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Evaluation & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Loss and Accuracy\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history.history['loss'], label='Train Loss')\n",
                "plt.plot(history.history['val_loss'], label='Val Loss')\n",
                "plt.title('Model Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history.history['accuracy'], label='Train Acc')\n",
                "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
                "plt.title('Model Accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.legend()\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on Test Set\n",
                "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
                "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
                "print(f\"Test Loss: {test_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save('model_1.h5')\n",
                "print(\"Model saved as model_1.h5\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}